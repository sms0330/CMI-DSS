{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "from sklearn.metrics import f1_score,accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series_ids = pd.read_parquet('./Dataset/Zzzs_train.parquet', columns=['series_id'])\n",
    "series_ids = series_ids.series_id.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocessing(df, window):\n",
    "    rog_args = {'window' : window, 'min_periods':2}\n",
    "    df[f'anglez_bf_{window}_std'] = df.anglez.rolling(**rog_args).std().round(4)\n",
    "    df[f'anglez_at_{window}_std'] = df.anglez.iloc[::-1].rolling(**rog_args).std().round(4).sort_index()\n",
    "    df[f'anglez_bf_{window}_mean'] = df.anglez.rolling(**rog_args).mean().round(4)\n",
    "    df[f'anglez_at_{window}_mean'] = df.anglez.iloc[::-1].rolling(**rog_args).mean().round(4).sort_index()\n",
    "    df[f'enmo_bf_{window}_std'] = df.enmo.rolling(**rog_args).std().round(4)\n",
    "    df[f'enmo_at_{window}_std'] = df.enmo.iloc[::-1].rolling(**rog_args).std().round(4).sort_index()\n",
    "    df[f'enmo_bf_{window}_mean'] = df.enmo.rolling(**rog_args).mean().round(4)\n",
    "    df[f'enmo_at_{window}_mean'] = df.enmo.iloc[::-1].rolling(**rog_args).mean().round(4).sort_index()\n",
    "    df.dropna(inplace=True)\n",
    "    df.reset_index(drop=True,inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_series_ids = series_ids\n",
    "train_list = []\n",
    "window_list = [60, 360, 720] # 5m, 30m, 1h\n",
    "for _id in tqdm(train_series_ids):\n",
    "    train_df_tmp = pd.read_parquet(\"./Dataset/Zzzs_train.parquet\", filters=[('series_id','=',_id)], columns = ['anglez', 'enmo', 'awake'])\n",
    "    train_df_tmp.anglez = (train_df_tmp.anglez + 8.8104) / 35.5218\n",
    "    train_df_tmp.enmo = (train_df_tmp.enmo - 0.0413) / 0.1018\n",
    "    for window in window_list:\n",
    "        train_df_tmp = data_preprocessing(train_df_tmp, window)\n",
    "    train_list.append(train_df_tmp)\n",
    "\n",
    "train = pd.concat(train_list, ignore_index=True)\n",
    "train_x = train.drop('awake',axis = 1)\n",
    "train_y = train[['awake']]\n",
    "del train, train_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원본 DataFrame의 열 이름 저장\n",
    "column_names = train_x.columns.tolist()\n",
    "\n",
    "# 시퀀스 길이를 정의\n",
    "sequence_length = 24  # 시퀀스 길이를 24로 설정\n",
    "\n",
    "# train_x의 첫 번째 차원이 sequence_length로 나누어 떨어지도록 train_x를 잘라냄\n",
    "train_x = train_x[:-(train_x.shape[0] % sequence_length)]\n",
    "\n",
    "# 데이터 형태 변경\n",
    "train_x = np.array(train_x).reshape(-1, sequence_length, train_x.shape[1])\n",
    "train_y = np.array(train_y)\n",
    "\n",
    "# 모델 정의\n",
    "model = Sequential()\n",
    "\n",
    "# LSTM 레이어 추가\n",
    "model.add(LSTM(128, input_shape=(sequence_length, train_x.shape[2]), activation='tanh', recurrent_activation='sigmoid', recurrent_dropout=0, return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(LSTM(128, activation='tanh', recurrent_activation='sigmoid', recurrent_dropout=0))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "\n",
    "# 출력 레이어 추가\n",
    "model.add(Dense(1, activation='sigmoid'))  # awake 필드가 이진 분류라고 가정\n",
    "\n",
    "# 모델 컴파일\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# 모델 학습\n",
    "model.fit(train_x, train_y, epochs=3, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_events(_id, model, file_root = None):\n",
    "    test = pd.read_parquet(f\"{file_root}\", filters=[('series_id','=',_id)])\n",
    "    test['timestamp'] = pd.to_datetime(test['timestamp']).apply(lambda t: t.tz_localize(None))\n",
    "    test['date'] = test['timestamp'].dt.date\n",
    "    test['hour'] = test['timestamp'].dt.hour\n",
    "    test.anglez = (test.anglez + 8.8104) / 35.5218\n",
    "    test.enmo = (test.enmo - 0.0413) / 0.1018\n",
    "    for window in window_list:\n",
    "        test = data_preprocessing(test,window)\n",
    "    test.dropna(inplace=True)\n",
    "    test.reset_index(drop=True,inplace=True)\n",
    "    test_x = test[column_names]\n",
    "    \n",
    "    # 데이터 형태 변경\n",
    "    test_x = np.array(test_x).reshape(-1, sequence_length, test_x.shape[1])\n",
    "\n",
    "    # RNN 모델을 사용한 예측\n",
    "    preds_probs = model.predict(test_x)\n",
    "    preds = np.round(preds_probs).flatten()  # 이진 분류 결과를 얻기 위해 반올림\n",
    "    probs = preds_probs.flatten()\n",
    "\n",
    "    # preds와 probs를 별도의 데이터프레임으로 생성\n",
    "    preds_df = pd.DataFrame(preds, columns=['prediction'])\n",
    "    probs_df = pd.DataFrame(probs, columns=['probability'])\n",
    "\n",
    "    # preds와 probs 데이터프레임을 test 데이터프레임과 병합\n",
    "    test = pd.concat([test, preds_df, probs_df], axis=1)\n",
    "    \n",
    "    test = test[test['prediction']!=2]\n",
    "    test.loc[test['prediction']==0, 'probability'] = 1-test.loc[test['prediction']==0, 'probability']\n",
    "    test['score'] = test['probability'].rolling(60*12*5, center=True, min_periods=10).mean().bfill().ffill()\n",
    "    test['pred_diff'] = test['prediction'].diff()\n",
    "    test['event'] = test['pred_diff'].replace({1:'wakeup', -1:'onset', 0:np.nan})\n",
    "    test.loc[test.hour.isin([0,1,2,3,4,5,6]),'date'] = test.loc[test.hour.isin([0,1,2,3,4,5,6]),'date'] + pd.Timedelta(days=-1)\n",
    "    test_wakeup = test[test['event']=='wakeup'].groupby(test['timestamp'].dt.date).agg('first')\n",
    "    test_onset = test[test['event']=='onset'].groupby(test['date']).agg('last')\n",
    "    test = pd.concat([test_wakeup, test_onset], ignore_index=True).sort_values('timestamp')\n",
    "    return test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측 및 제출 파일 생성\n",
    "file_root = './Dataset/test_series.parquet'\n",
    "series_id  = pd.read_parquet(file_root, columns=['series_id'])\n",
    "series_id = series_id.series_id.unique()\n",
    "submit_columns = ['series_id','step','event','score']\n",
    "submission = []\n",
    "\n",
    "for _id in series_id:\n",
    "    test_tmp = get_events(_id, model, file_root)\n",
    "    test_tmp = test_tmp[submit_columns]\n",
    "    submission.append(test_tmp)\n",
    "\n",
    "submission = pd.concat(submission, ignore_index=True).reset_index(names='row_id')\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
